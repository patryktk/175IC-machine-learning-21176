{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrVMlm7DEZdDaKYtpRm0th",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patryktk/175IC-machine-learning-21176/blob/main/Lab5/lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrHlPsw7LhJW"
      },
      "source": [
        "!pip install spacy --upgrade\n",
        "!python -m spacy download pl_core_news_sm\n",
        "!pip install textacy\n",
        "import spacy\n",
        "import pl_core_news_sm\n",
        "import re\n",
        "from spacy.tokenizer import Tokenizer\n",
        "nlp = pl_core_news_sm.load()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH0TCGqDeOYm"
      },
      "source": [
        "#String Read\n",
        "about_text = ('Tutorial spacy do laboratrium, który jest dość długi, żeby sprawdzić inforacje')\n",
        "about_doc = nlp(about_text)\n",
        "print([token.text for token in about_doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWir8Qb8f9z0"
      },
      "source": [
        "#Text file read\n",
        "file_name = 'Notatnik.txt'\n",
        "file_text = open(file_name).read()\n",
        "file_doc = nlp(file_text)\n",
        "#Extract token\n",
        "print([token.text for token in file_doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXfNKujJnCMg"
      },
      "source": [
        "sentences =list(about_doc.sents)\n",
        "len(sentences)\n",
        "for sentences in sentences:\n",
        "  print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH6k9Ehu8Z6o"
      },
      "source": [
        "def kropki(doc): #ta funkcja powoduje że w momencie wyczytania w tokenie '...' przenosimy tekst do kolejnej lini\n",
        "  for token in doc[:-1]:\n",
        "    if token.text == '...':\n",
        "      doc[token.i+1].is_sent_start = True\n",
        "  return doc\n",
        "\n",
        "text_kropki = ('To jest tekst ... zaweierajacy'\n",
        "              'duza ilosc ... kropek.')\n",
        "custom_nlp = pl_core_news_sm.load() #ładujemy nasz język\n",
        "custom_nlp.add_pipe(kropki, before='parser') #edytujemy nasz tekst i zwracamy tekest przed parserem\n",
        "custom_ell_doc = custom_nlp(text_kropki) #ładujemy nasz tekst do zmiennej i \n",
        "custom_ell_sent = list(custom_ell_doc.sents) #elementy łączymy w liste, aby tekst był spójny\n",
        "for sentences in custom_ell_sent:\n",
        "  print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nnFbC-lGEkx"
      },
      "source": [
        "#Tokenization\n",
        "\n",
        "for token in about_doc:\n",
        "  print (token, token.idx, token.text_with_ws,\n",
        "        token.is_alpha, token.is_punct, token.is_space,\n",
        "         #is_alpha - sprawdza czy znaki są z alfabetu\n",
        "         #is_punct - sprawdza znaki interpunkcyjne\n",
        "         #is_space szuka spacji\n",
        "         #shape - kształt słowa, rozmiar liter\n",
        "         #is_stop-  sprawdza czy to stop word czy nie\n",
        "        token.shape_, token.is_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gacfE4U8CvBE"
      },
      "source": [
        "#Tworzenie własnego tokenu\n",
        "prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
        "suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n",
        "infix_re = re.compile(r'''[-~]''')\n",
        "def customize_tokenizer(nlp):\n",
        "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,suffix_search=suffix_re.search,infix_finditer=infix_re.finditer,token_match=None)\n",
        "custom_token_text = 'To-jest-mój-tokenizer'\n",
        "nlp.tokenizer = customize_tokenizer(nlp)\n",
        "custom_tokenizer_doc = nlp(custom_token_text)\n",
        "print([token.text for token in custom_tokenizer_doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs776xgFEjAr"
      },
      "source": [
        "spacy_stopwords = spacy.lang.pl.stop_words.STOP_WORDS\n",
        "len(spacy_stopwords)\n",
        "\n",
        "for stop_word in list(spacy_stopwords)[:10]:\n",
        "  print(stop_word)\n",
        "\n",
        "#usuwanie stop wordow\n",
        "for token in about_doc:\n",
        "  if not token.is_stop:\n",
        "    print (token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN0FHyCYJJL3"
      },
      "source": [
        "# sprowadzanie słów do form podstawowych\n",
        "lemmatization_text = ('To jest jak przykładowy przykład przykładu działania przykładu')\n",
        "lemmatization_doc = nlp(lemmatization_text)\n",
        "for token in lemmatization_doc:\n",
        "  print(token, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UcbncsHJxsa"
      },
      "source": [
        "#Częstotliwość występownaia słów w tekście\n",
        "from collections import Counter\n",
        "complete_text=(about_text)\n",
        "complete_doc = nlp(about_text)\n",
        "\n",
        "words = [token.text for token in complete_doc\n",
        "         if not token.is_stop and not token.is_punct]\n",
        "word_freq = Counter(words)\n",
        "\n",
        "#Najczęstsze słowa\n",
        "common_words = word_freq.most_common(5)\n",
        "print (common_words)\n",
        "\n",
        "#Słowa unikatowe\n",
        "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
        "print (unique_words)\n",
        "\n",
        "#Ropoznanie części mowy\n",
        "for token in complete_doc:\n",
        "  print(token, token.tag_, token.pos_, spacy.explain(token.tag_),sep=', ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E7yyRaQLYHW",
        "outputId": "755d7dad-f4ee-4879-f0de-1c04582aa09d"
      },
      "source": [
        "#Kategoryzacja\n",
        "nouns = []\n",
        "adjectives = []\n",
        "for token in about_doc:\n",
        "    if token.pos_ == 'NOUN':\n",
        "        nouns.append(token)\n",
        "    if token.pos_ == 'ADJ':\n",
        "        adjectives.append(token)\n",
        "\n",
        "print(f'Rzeczowniki: {nouns}')\n",
        "print(f'Przymiotniki: {adjectives}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rzeczowniki: [Tutorial, spacy, laboratrium, inforacje]\n",
            "Przymiotniki: [który, długi]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHgEXj8XdlK6"
      },
      "source": [
        "#Wizualizacja\n",
        "from spacy import displacy\n",
        "about_displaCy_text = ('Laborka robiona na ostatnia chwile')\n",
        "about_displaCy_doc = nlp(about_displaCy_text)\n",
        "displacy.render(about_displaCy_doc, style='dep', jupyter=True)\n",
        "#Aby wyświetlić w jupiter notebooku, z którego korzystam używaym ww. komendy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awdYOzdJeMvz"
      },
      "source": [
        "#przetwarzanie tekstu, na format łatwy do analizy przez NLP\n",
        "def is_token_allowed(token):\n",
        "    if not token or not token.string.strip() or token.is_stop or token.is_punct:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def preprocess_token(token):\n",
        "    return token.lemma_.strip().lower()\n",
        "\n",
        "complete_filtered_tokens = [preprocess_token(token)for token in complete_doc if is_token_allowed(token)]\n",
        "print(complete_filtered_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dlf_YQHfJpk"
      },
      "source": [
        "#Wyciąganie imienia i nazwisko, ale niestety nie działa\n",
        "from spacy.matcher import Matcher\n",
        "def extract_full_name(nlp_doc):\n",
        "    pattern = [{'POS': 'PROPN '}, {'POS': 'PROPN'}]\n",
        "    matcher.add('FULL_NAME', None, pattern)\n",
        "    matches = matcher(nlp_doc)\n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_doc[start:end]\n",
        "        return span.text\n",
        "\n",
        "new_text_extract=\"Patryk Tkaczyk jedzenie to jest test Test\"\n",
        "new_text_extract_doc=nlp(new_text_extract)\n",
        "matcher = Matcher(nlp.vocab)\n",
        "print(extract_full_name(new_text_extract_doc))\n",
        "\n",
        "new_text_extract_eg=\"Gus Proto\"\n",
        "new_text_extract_doc_eg=nlp(new_text_extract_eg)\n",
        "matcher = Matcher(nlp.vocab)\n",
        "print(extract_full_name(new_text_extract_doc_eg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vVhQXt0fet7"
      },
      "source": [
        "#Wyciąganie numeru telefonu\n",
        "phone_number=(\"To mój numer: 666 668 152\")\n",
        "\n",
        "def extract_phone_number(nlp_doc):\n",
        "\tpattern = [{\"SHAPE\": \"ddd\"}, {\"SHAPE\": \"ddd\"},{\"SHAPE\": \"ddd\"}]\n",
        "\tmatcher.add('PHONE_NUMBER', None, pattern)\n",
        "\tmatches = matcher(nlp_doc)\n",
        "\tfor match_id, start, end in matches:\n",
        "\t\tspan = nlp_doc[start:end]\n",
        "\t\treturn span.text\n",
        "\n",
        "conference_org_doc = nlp(phone_number)\n",
        "extract_phone_number(conference_org_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6gGOpmcfwkc"
      },
      "source": [
        "#Strukutra zdania:\n",
        "text = ('Przykładowy tekest to tekst jest')\n",
        "text_doc = nlp(text)\n",
        "for token in text_doc:\n",
        "  print (token.text, token.tag_, token.head.text, token.dep_)\n",
        "displacy.render(text_doc, style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd1TdhY9f_tW"
      },
      "source": [
        "#Nawigacja do drzew\n",
        "one_line_about_text = ('Tańczymy jakby jutro miało nie być, myslovitz')\n",
        "one_line_about_doc = nlp(one_line_about_text)\n",
        "\n",
        "#Podzielnie 3 słowa oraz wyświetlenie sąsiadów słowa\n",
        "print([token.text for token in one_line_about_doc[3].children])\n",
        "print (one_line_about_doc[3].nbor(-1))\n",
        "print (one_line_about_doc[3].nbor())\n",
        "#Poddrzewao słowa 'jutro'\n",
        "print (list(one_line_about_doc[3].subtree))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKwFbPI0gbpc"
      },
      "source": [
        "#Wykrywanie czasowników\n",
        "import textacy\n",
        "about_talk_text=('Grupa kolegów chodziła całą noc, po osiedlu i rzucali kamieniami w okna.')\n",
        "pattern = [{\"POS\": \"VERB\", \"OP\": \"*\"},{\"POS\": \"ADV\", \"OP\": \"*\"},{\"POS\": \"VERB\", \"OP\": \"+\"},{\"POS\": \"PART\", \"OP\": \"*\"}]\n",
        "about_talk_doc = textacy.make_spacy_doc(about_talk_text, nlp)\n",
        "verb_phrases = textacy.extract.matches(about_talk_doc, pattern)\n",
        "for chunk in verb_phrases:\n",
        "    print(chunk.text)\n",
        "\n",
        "for chunk in about_talk_doc.noun_chunks:\n",
        "    print (chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSiOzA-_g17z"
      },
      "source": [
        "#Używanie NER do wyszukiwania konkretnych danych i ukrywania ich\n",
        "\n",
        "survey_text =('Jan Nowak nie wie co się dzieje natomiast Anna Dymna doskonale zdaje sobie sprawę, że Tomasz Kos zjadł jej czekoaldki')\n",
        "def replace_person_names(token):\n",
        "    if token.ent_iob != 0 and token.ent_type_ == 'persName':\n",
        "        return '[Zgadnij] '\n",
        "    return token.string\n",
        "\n",
        "def redact_names(nlp_doc):\n",
        "    for ent in nlp_doc.ents:\n",
        "        ent.merge()\n",
        "    tokens = map(replace_person_names, nlp_doc)\n",
        "    return ''.join(tokens)\n",
        "\n",
        "survey_doc = nlp(survey_text)\n",
        "print(redact_names(survey_doc))\n",
        "print(survey_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}